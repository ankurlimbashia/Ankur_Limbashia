{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## making functions required for decision tree and storing the output in a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "#entropy fuction\n",
    "def Entropy(y):\n",
    "    value,count = np.unique(y,return_counts = True)  # to get the unique value and its count\n",
    "    entropy=0\n",
    "    for i in range(len(value)):\n",
    "        a= count[i]/count.sum()\n",
    "        entropy += (-a)*math.log2(a)\n",
    "    \n",
    "    return entropy\n",
    "\n",
    "# for weighted entrop after splitting as per specific feature\n",
    "def weighted_entropy(x,feature,y):       \n",
    "    w_entropy = 0\n",
    "    value,count= np.unique(x[feature],return_counts=True)  # to get the unique value and its count\n",
    "    for i in range(len(value)):\n",
    "        a= Entropy(y[x[feature] == value[i]].dropna())  # getting the value of those y whose index is matches the x[feature]==value[i]\n",
    "        w_entropy += (count[i]/count.sum())*a\n",
    "    return w_entropy\n",
    "\n",
    "# function to find information gain\n",
    "def info_gain(x,feature,y):\n",
    "    total_entropy = Entropy(y)\n",
    "    Information_Gain = total_entropy - weighted_entropy(x,feature,y)\n",
    "    return Information_Gain\n",
    "\n",
    "# function to find split information\n",
    "def split_ratio(x,feature):\n",
    "    value,count = np.unique(x[feature],return_counts = True)  # to get the unique value and its count\n",
    "    entropy=0\n",
    "    for i in range(len(value)):\n",
    "        a= count[i]/count.sum()\n",
    "        entropy += (-a)*math.log2(a)\n",
    "    return entropy\n",
    "\n",
    "# function to find gain ratio\n",
    "def gain_ratio(info_gain,x,feature):\n",
    "    ratio =  info_gain/split_ratio(x,feature)\n",
    "    return ratio\n",
    "\n",
    "# function for decision tree which will print the required ouput and also return a tree like dictionary\n",
    "def DecisionTree(x,y,features,target_name,main = None,level=0):    \n",
    " \n",
    "    if len(np.unique(y)) <= 1:   # checking if leaf node is reached\n",
    "        value,count = np.unique(y,return_counts = True) # to get the unique value and its count\n",
    "        print('Level ',level)\n",
    "        print('count of',target_name[int(value[0])],'=',count[0]) # printing the target name and its count\n",
    "        print('Current Entropy is =',Entropy(y)) # pritnting current entropy\n",
    "        print('Reached Leaf Node ')\n",
    "        print()\n",
    "        \n",
    "        v,c = np.unique(y,return_counts = True)\n",
    "        return target_name[int(v[0])],c[0] # returning the target name and its count so that is can be store inside tree dictionary\n",
    "        \n",
    "    elif len(features) == 0 :  # checking if features for further split are available or not\n",
    "        return main # returning the unique value in target as there is no further splitting is possible\n",
    "    \n",
    "    else :\n",
    "\n",
    "        main = []\n",
    "        v,c = np.unique(y,return_counts = True)\n",
    "        for i in range(len(v)):\n",
    "            main.append([target_name[int(v[i])],c[i]]) # getting all the unique data of target name\n",
    "        \n",
    "        # checking over all the feature to get the best feature having the max information gain\n",
    "        values = []\n",
    "        for f in features :                        \n",
    "            v = info_gain(x,f,y)     # making the list of information gain for all features\n",
    "            values.append(v)\n",
    "        \n",
    "        \n",
    "        best_feature_index = np.argmax(values)        \n",
    "        best_feature = features[best_feature_index] # getting the best feature amongst all\n",
    "        \n",
    "        ratio_gain = gain_ratio(max(values),x,best_feature) # getting the gain ratio for the best feature upon which splitting has to be done\n",
    "        \n",
    "        tree = {best_feature:{}} # dictionary to store decision tree\n",
    "        \n",
    "        tot_entropy = Entropy(y)   # gettnig total entropy for current level\n",
    "       \n",
    "                \n",
    "        value,count = np.unique(y,return_counts = True) # to get the unique value and its count\n",
    "        print('Level ',level)  \n",
    "        # printing the target name and its count at given level\n",
    "        for i in range(len(value)):\n",
    "            if value[i]==0 :\n",
    "                print('count of',target_name[int(value[i])]  ,'=',count[i])\n",
    "            elif value[i]==1 :\n",
    "                print('count of',target_name[int(value[i])]  ,'=',count[i])\n",
    "            elif value[i]==2 :\n",
    "                print('count of',target_name[int(value[i])]  ,'=',count[i])\n",
    "      \n",
    "        print('Current entropy is   = ',tot_entropy)  # printing total entropy of current level\n",
    "        \n",
    "        print('Splitting on feature ',best_feature,' with gain ratio ', ratio_gain) # printing the gain raion with the splitting feature\n",
    "        \n",
    "        print()\n",
    "        \n",
    "        # removing the feature after splitting has been done\n",
    "        new_features = features         \n",
    "        features=[]\n",
    "        for i in new_features :         \n",
    "            if i != best_feature :                  \n",
    "                features.append(i)\n",
    "        level += 1       \n",
    "        new_features=None   # removed the feature which has splitted                  \n",
    "        \n",
    "        # for all different value in that particular splitting feature again \n",
    "        for i in np.unique(x[best_feature]):  \n",
    "            x_new = (x.where(x[best_feature] == i)).dropna()\n",
    "            y_new = (y.where(x[best_feature] == i)).dropna()\n",
    "            new_tree = DecisionTree(x_new,y_new,features,target_name,main,level) # making tree for other feature\n",
    "            tree[best_feature][i] = new_tree #updating the tree dictionary\n",
    "        \n",
    "        return tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## importing data and classifying the data as we have continuous data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn import datasets\n",
    "pd.options.mode.chained_assignment = None  # to remove the error while coping data in dataframe\n",
    "\n",
    "data = datasets.load_iris() #loading IRIS datasets\n",
    "\n",
    "x = pd.DataFrame(data.data) #taking all the input value in x and then giving them title\n",
    "x.columns = [\"sl\", \"sw\", 'pl', 'pw']\n",
    "y = pd.DataFrame(data.target) # taking the output in Y \n",
    "y.columns = ['target']\n",
    "target_name = data.target_names # all the target name\n",
    "feature_name=x.columns #all the feature name\n",
    "\n",
    "# a function which will classify the continuous data present in input dataframe\n",
    "# here classification is done by 4 parts\n",
    "\n",
    "def classify(df, col):\n",
    "    second = df[col].mean() #second boundary\n",
    "    minimum = df[col].min()\n",
    "    first = (minimum + second)/2 #first boundary\n",
    "    maximum = df[col].max()\n",
    "    third = (maximum + second)/2 # third Boundary\n",
    "\n",
    "# checking each value in column of dataset and using boundaries changing the value and hence making a classified column\n",
    " \n",
    "    for i in range(len(df[col])): \n",
    "        if df[col][i]<first:\n",
    "            df[col][i] = 'a'\n",
    "        elif df[col][i]<second:\n",
    "            df[col][i] = 'b'\n",
    "        elif df[col][i]<third:\n",
    "            df[col][i] = 'c'\n",
    "        else:\n",
    "            df[col][i] = 'd'\n",
    "            \n",
    "    return\n",
    "\n",
    "# using classify function for classification\n",
    "classify(x, 'sl')\n",
    "classify(x, 'sw')\n",
    "classify(x, 'pl')\n",
    "classify(x, 'pw')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## output for whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level  0\n",
      "count of setosa = 50\n",
      "count of versicolor = 50\n",
      "count of virginica = 50\n",
      "Current entropy is   =  1.584962500721156\n",
      "Splitting on feature  pw  with gain ratio  0.6996382036222091\n",
      "\n",
      "Level  1\n",
      "count of setosa = 50\n",
      "Current Entropy is = 0.0\n",
      "Reached Leaf Node \n",
      "\n",
      "Level  1\n",
      "count of versicolor = 10\n",
      "Current Entropy is = 0.0\n",
      "Reached Leaf Node \n",
      "\n",
      "Level  1\n",
      "count of versicolor = 40\n",
      "count of virginica = 16\n",
      "Current entropy is   =  0.863120568566631\n",
      "Splitting on feature  pl  with gain ratio  0.4334099495621067\n",
      "\n",
      "Level  2\n",
      "count of versicolor = 1\n",
      "Current Entropy is = 0.0\n",
      "Reached Leaf Node \n",
      "\n",
      "Level  2\n",
      "count of versicolor = 39\n",
      "count of virginica = 8\n",
      "Current entropy is   =  0.6581912658132185\n",
      "Splitting on feature  sl  with gain ratio  0.12674503775809332\n",
      "\n",
      "Level  3\n",
      "count of virginica = 1\n",
      "Current Entropy is = 0.0\n",
      "Reached Leaf Node \n",
      "\n",
      "Level  3\n",
      "count of versicolor = 14\n",
      "Current Entropy is = 0.0\n",
      "Reached Leaf Node \n",
      "\n",
      "Level  3\n",
      "count of versicolor = 23\n",
      "count of virginica = 7\n",
      "Current entropy is   =  0.783776947484701\n",
      "Splitting on feature  sw  with gain ratio  0.07092036405148876\n",
      "\n",
      "Level  4\n",
      "count of versicolor = 6\n",
      "Current Entropy is = 0.0\n",
      "Reached Leaf Node \n",
      "\n",
      "Level  3\n",
      "count of versicolor = 2\n",
      "Current Entropy is = 0.0\n",
      "Reached Leaf Node \n",
      "\n",
      "Level  2\n",
      "count of virginica = 8\n",
      "Current Entropy is = 0.0\n",
      "Reached Leaf Node \n",
      "\n",
      "Level  1\n",
      "count of virginica = 34\n",
      "Current Entropy is = 0.0\n",
      "Reached Leaf Node \n",
      "\n"
     ]
    }
   ],
   "source": [
    "tree=DecisionTree(x,y,feature_name,target_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pw': {'a': ('setosa', 50),\n",
       "  'b': ('versicolor', 10),\n",
       "  'c': {'pl': {'b': ('versicolor', 1),\n",
       "    'c': {'sl': {'a': ('virginica', 1),\n",
       "      'b': ('versicolor', 14),\n",
       "      'c': {'sw': {'a': [['versicolor', 23], ['virginica', 7]],\n",
       "        'b': [['versicolor', 23], ['virginica', 7]],\n",
       "        'c': ('versicolor', 6)}},\n",
       "      'd': ('versicolor', 2)}},\n",
       "    'd': ('virginica', 8)}},\n",
       "  'd': ('virginica', 34)}}"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
